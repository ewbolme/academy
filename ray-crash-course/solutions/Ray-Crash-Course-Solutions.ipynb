{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Crash Course - Exercise Solutions\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "This notebook discusses solutions for the exercises in the _crash course_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Ray Crash Course - Tasks - Exercise 1\n",
    "\n",
    "As currently written, the memory footprint of `estimate_pi` scales linearly with `N`, because it allocates two NumPy arrays of size `N`. This limits the size of `N` we can evaluate (as I confirmed by locking up my laptop...). However, this isn't actually necessary. We could do the same calculation in \"blocks, for example `m` blocks of size `N/m` and then combine the results. Furthermore, there's no dependencies between the calculations with those blocks, giving us further potential speed-up by parellelizing them with Ray.\n",
    "\n",
    "Adapt `ray_estimate_pi` to use this technique. Pick some `N` value above which the calculation is done in blocks. Compare the performance of the old vs. new implementation. \n",
    "\n",
    "As you do this exercise, you might ponder the fact that we often averaged multiple trials for a given `N` and then ask yourself, what's the difference between averaging `10` trials for `N = 1000` vs. `1` trial for `N = 10000`, for example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import things we need and redefine functions and data we need from the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, time, statistics, math\n",
    "import ray\n",
    "sys.path.append('..')\n",
    "from pi_calc import str_large_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dashboard URL: http://{ray.get_webui_url()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's `estimate_pi` again, but now we'll also return the counts, for reasons we'll discuss shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi(num_samples):\n",
    "    xs = np.random.uniform(low=-1.0, high=1.0, size=num_samples)   # Generate num_samples random samples for the x coordinate.\n",
    "    ys = np.random.uniform(low=-1.0, high=1.0, size=num_samples)   # Generate num_samples random samples for the y coordinate.\n",
    "    xys = np.stack((xs, ys), axis=-1)                              # Like Python's \"zip(a,b)\"; creates np.array([(x1,y1), (x2,y2), ...]).\n",
    "    inside = xs*xs + ys*ys <= 1.0                                  # Creates a predicate over all the array elements.\n",
    "    xys_inside = xys[inside]                                       # Selects only those \"zipped\" array elements inside the circle.\n",
    "    in_circle = xys_inside.shape[0]                                # Return the number of elements inside the circle.\n",
    "    approx_pi = 4.0*in_circle/num_samples                          # The Pi estimate.\n",
    "    return approx_pi, in_circle, num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the original `ray_estimate_pi`, but now it will also return the counts, not just $\\pi$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def ray_estimate_pi(num_samples):\n",
    "    return estimate_pi(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = '{:10.5f} seconds: pi ~ {:7.6f}, stddev = {:5.4f}, error = {:5.4f}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's `ray_try_it`, but now we handle the additional returned values from `ray_estimate_pi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ray_try_it(n, trials):\n",
    "    print('trials = {:5d}, N = {:s}: '.format(trials, str_large_n(n, padding=15)), end='')   # str_large_n imported above.\n",
    "    start = time.time()\n",
    "    refs = [ray_estimate_pi.remote(n) for _ in range(trials)]\n",
    "    pis_counts = ray.get(refs)\n",
    "    pis = list(map(lambda t: t[0], pis_counts))\n",
    "    approx_pi = statistics.mean(pis)\n",
    "    stdev = 0.0 if trials == 1 else statistics.stdev(pis)\n",
    "    duration = time.time() - start\n",
    "    error = (100.0*abs(approx_pi-np.pi)/np.pi)\n",
    "    print(fmt.format(duration, approx_pi, stdev, error))   # str_large_n imported above.\n",
    "    return trials, n, duration, approx_pi, stdev, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the \"ponder\" question at the end, just using the original implementation. We'll do a few runs of the following cell. Note that we're using large maximum `n` values here. If you are working on a slow machine or VM, consider deleting the last value `10000000` here and below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1000, 10000, 100000, 1000000, 10000000]:\n",
    "    ray_try_it(n, round(10000000/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1000, 10000, 100000, 1000000, 10000000]:\n",
    "    ray_try_it(n, round(10000000/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1000, 10000, 100000, 1000000, 10000000]:\n",
    "    ray_try_it(n, round(10000000/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation is misleading now, because the number of trials change. The errors are roughly within an order of magnitude, due in part to expected statistical variation. Generally speaking, larger `N` and lower `trials` had lower errors. This may be due to the other big source of variation, the inevitable rounding error computing $\\pi$ (`4 * inside_count/N`), one time per trial (`1` to `10,000` times). Experiments are supposed to eliminate as many extraneous variables as possible, so I would argue that sticking to one value for `trials` and varying `N` is more meaningful. In fact, in the implementation that follows, we'll eliminate the potential rounding error variation by keep track of the inside and total counts, then computing $\\pi$ once at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a function to return sample sizes for a given `N` and `m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sizes(N, m):\n",
    "    ranges = [(m*i, m*(i+1)) for i in range(math.ceil(N/m))]\n",
    "    if ranges[-1][1] > N:\n",
    "        ranges[-1] = (ranges[-1][0], N)\n",
    "    return list(map(lambda x: x[1]-x[0], ranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def ray_estimate_pi_blocks(num_samples, m):\n",
    "    \"\"\"\n",
    "    Perform the estimate in blocks up to ``m`` samples in size. A more user-friendly solution would embed logic to\n",
    "    determine an reasonably good ``m`` value, but for our purposes, passing in ``m`` is more convenient.\n",
    "    \"\"\"\n",
    "    sizes = sample_sizes(num_samples, m)\n",
    "    refs = [ray_estimate_pi.remote(size) for size in sizes]\n",
    "    values = ray.get(refs)  # Not using ray.wait() is okay; the tasks are all roughly the same size\n",
    "    inside_count = 0\n",
    "    total_count = 0\n",
    "    for _, icount, tcount in values:    # Toss the pi value returned\n",
    "        inside_count += icount\n",
    "        total_count += tcount\n",
    "    return 4.0*inside_count/total_count, inside_count, total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [10000, 100000, 1000000]:\n",
    "    print(f'm = {m}:')\n",
    "    for n in [1000, 10000, 100000, 1000000, 10000000, 100000000]:\n",
    "        start = time.time()\n",
    "        approx_pi, inside_count, total_count = ray.get(ray_estimate_pi_blocks.remote(n, 100000))\n",
    "        duration = time.time() - start\n",
    "        print(f'{n:15}: duration = {duration:6.5} seconds, pi = {approx_pi:6.5}, # inside/outside = {inside_count:12}/{total_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to the original implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1000, 10000, 100000, 1000000, 10000000, 100000000]:\n",
    "    start = time.time()\n",
    "    approx_pi, inside_count, total_count = ray.get(ray_estimate_pi.remote(n))\n",
    "    duration = time.time() - start\n",
    "    print(f'{n:15}: duration = {duration:6.5} seconds, pi = {approx_pi:6.5}, # inside/outside = {inside_count:12}/{total_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for larger `N`, `ray_estimate_pi_blocks` time scale noticeably slower than the original implementation, e.g., for the highest `N`, `100,000,000`, the durations are approximately `1.2` seconds vs. `9.6` seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Ray Crash Course - Tasks - Exercise 2\n",
    "\n",
    "What `N` value is needed to get a reliable estimate to five decimal places, `3.1415` (for some definition of \"reliable\")? If you have a powerful machine or a cluster, you could try a higher accuracy. You'll need to use the solution to Exercise 1 or you can make a guess based on the results we've already seen in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the solution from Exercise 1, we'll need a modified `ray_try_it` to add the `m` blocks parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ray_try_it_blocks(n, m, trials):\n",
    "    print('trials = {:5d}, N = {:s}: '.format(trials, str_large_n(n, padding=15)), end='')   # str_large_n imported above.\n",
    "    start = time.time()\n",
    "    refs = [ray_estimate_pi_blocks.remote(n, m) for _ in range(trials)]\n",
    "    pis_counts = ray.get(refs)\n",
    "    pis = list(map(lambda t: t[0], pis_counts))\n",
    "    approx_pi = statistics.mean(pis)\n",
    "    stdev = 0.0 if trials == 1 else statistics.stdev(pis)\n",
    "    duration = time.time() - start\n",
    "    error = (100.0*abs(approx_pi-np.pi)/np.pi)\n",
    "    print(fmt.format(duration, approx_pi, stdev, error))   # str_large_n imported above.\n",
    "    return trials, n, duration, approx_pi, stdev, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the error we would have to achieve for this accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_error = 100*abs(3.1415 - np.pi)/np.pi\n",
    "target_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's keep trying bigger `N` until we get to this number, but now we need to pick a definition of \"reliable\", because the results will depend on the number of `trials` we do. Also, some experiments will get \"lucky\" for relatively low `N` values.\n",
    "\n",
    "> **WARNING:** This could take a while. You could choose a less accurate error goal if you have limited compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "error = 10.0\n",
    "while error > target_error:\n",
    "    N *= 10\n",
    "    _, _, duration, approx_pi, _, error = ray_try_it_blocks(N, 1000000, trials)\n",
    "    if N > 100000000:\n",
    "        print(\"Stopping so we don't crash the machine...\")\n",
    "        break\n",
    "print(f'{N} samples is sufficient to get the error below {target_error}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should run the previous cell several times. Some runs might succeed with `N = 100,000`, while more often it will be above 1M or 10M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Ray Crash Course - Tasks - Exercise 3\n",
    "\n",
    "For small computation problems, Ray adds enough overhead that its benefits are outweighed. You can see from the performance graphs in the lesson that smaller `N` or smaller trial values will likely cause the performance curves to cross. Try small values of `N` and small trial numbers. When do the lines cross? Try timing individual runs for small `N` around the crossing point. What can you infer from this \"tipping point\" about appropriate sizing of tasks, at least for your test environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, here is more code from the notebook. Here is `try_it`, modified to handle the extra return values from the modified `estimate_pi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_it(n, trials):\n",
    "    print('trials = {:3d}, N = {:s}: '.format(trials, str_large_n(n, padding=12)), end='')   # str_large_n imported above.\n",
    "    start = time.time()\n",
    "    pis_counts = [estimate_pi(n) for _ in range(trials)]\n",
    "    pis = list(map(lambda t: t[0], pis_counts))\n",
    "    approx_pi = statistics.mean(pis)\n",
    "    stdev = statistics.stdev(pis)\n",
    "    duration = time.time() - start\n",
    "    error = (100.0*abs(approx_pi-np.pi)/np.pi)\n",
    "    print(fmt.format(duration, approx_pi, stdev, error))   # str_large_n imported above.\n",
    "    return trials, n, duration, approx_pi, stdev, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ns = [1, 10, 100, 1000, 10000, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ns = [try_it(n, trials) for n in small_ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_data_ns = [ray_try_it(n, trials) for n in small_ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data_ns         = np.array(data_ns)\n",
    "np_ray_data_ns     = np.array(ray_data_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh_util import two_lines_plot, means_stddevs_plot  # Some plotting utilities in `./bokeh_util.py`.\n",
    "from bokeh.plotting import show, figure\n",
    "from bokeh.layouts import gridplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_lines = two_lines_plot(\n",
    "    \"N vs. Execution Times (Smaller Is Better)\", 'N', 'Time', 'No Ray', 'Ray', \n",
    "    np_data_ns[:,1], np_data_ns[:,2], np_ray_data_ns[:,1], np_ray_data_ns[:,2],\n",
    "    x_axis_type='log', y_axis_type='log')\n",
    "show(two_lines, plot_width=800, plot_height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you can't see it, click [here](../../images/Pi-small-Ns-vs-times.png).)\n",
    "\n",
    "Let's calculate the `N` where they cross:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(small_ns)):\n",
    "    if data_ns[i] >= ray_data_ns[i]:\n",
    "        print(f'Crossing point: N = {small_ns[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Ray Crash Course - Actors - Exercise 1\n",
    "\n",
    "You are asked these questions about the `Counter` vs. `RayCounter` performance:\n",
    "\n",
    "> Ignoring pause = 0, can you explain why the Ray times are almost, but slightly larger than the non-ray times consistently? Study the implementations for `ray_counter_trial` and `RayCounter`. What code is synchronous and blocking vs. concurrent? In fact, is there _any_ code that is actually concurrent when you have just one instance of `Counter` or `RayCounter`?\n",
    "\n",
    "Here is `ray_counter_trial` again, with comments about concurrency vs. synchronous blocking calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ray_counter_trial(count_to, num_counters = 1, pause = 0.01):\n",
    "    print('ray:     count_to = {:5d}, num counters = {:4d}, pause = {:5.3f}: '.format(count_to, num_counters, pause), end='')\n",
    "    start = time.time()\n",
    "    final_count_futures = []\n",
    "    # Actor instantiation blocks, but returns almost immediately. The actor creation overhead is low. It is a little bit larger\n",
    "    # than normal class instantiation, but insignificant for overall performance.\n",
    "    counters = [RayCounter.remote(pause) for _ in range(num_counters)]\n",
    "    for i in range(num_counters):\n",
    "        for n in range(count_to):\n",
    "            counters[i].next.remote()                                # Nonblocking, so will be faster for long pause scenarios...\n",
    "        final_count_futures.append(counters[i].get_count.remote())  \n",
    "    ray.get(final_count_futures)                                     # but block until all invocations are finished!\n",
    "    duration = time.time() - start\n",
    "    print('time = {:9.5f} seconds'.format(duration))\n",
    "    return count_to, num_counters, pause, duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `next` methods in `Counter` and `RayCounter`, call `time.sleep(pause)` before completing, but for `RayCounter` it runs asynchronously, while it blocks for `Counter`. You do have to block to get the current count and if lots of async invocations of `next` are being processed, a call to `ray.get(actor.get_counter())` will block until all of them are finished.\n",
    "\n",
    "Hence, the reason a single `RayCounter` instance never outperforms a `Counter` instance is because _all_ the code in `ray_counter_trial` becomes effectively _synchronous_ because of the single line `ray.get(final_count_futures)`. Since the Ray implementation has extra overhead for Ray, it will always take a little longer.\n",
    "\n",
    "The real benefit is running many counters concurrently. `ray_counter_trial` does this seamlessly, while `counter_trial` remains fully synchronous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the exercise is this statement and question:\n",
    "\n",
    "> Once past zero pauses, the Ray overhead is constant. It doesn't grow with the pause time. Can you explain why it doesn't grow?\n",
    "\n",
    "The Ray overhead doesn't change because the number of Ray-related invocations don't change as the pause time grows. We still use one counter instance and ten invocations of it. Hence the overhead is a constant, even though the method invocations will take longer to complete, depending on the `pause` value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Ray Crash Course - Why Ray?\n",
    "\n",
    "There were no exercises for this lesson.\n",
    "\n",
    "# 04 Ray Crash Course - Python Multiprocessing with Ray\n",
    "\n",
    "There were no exercises for this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Ray Crash Course - Ray Parallel Iterators - Exercises 1-3\n",
    "\n",
    "Here we combine the solutions for the first three exercises. This code is also available as a complete, standalone Ray program in [word-count-exercises.py](word-count-exercises.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, gzip, re, sys, os\n",
    "import numpy as np\n",
    "\n",
    "class WordCount:\n",
    "    \"Wraps a dictionary of words and counts.\"\n",
    "    def __init__(self):\n",
    "        self.counts = {}\n",
    "\n",
    "    def __call__(self, word, increment):\n",
    "        count = increment\n",
    "        if word in self.counts:\n",
    "            count = self.counts[word]+increment\n",
    "        self.counts[word] = count\n",
    "        return (word, count)\n",
    "\n",
    "    def sort_counts(self, descending=True):\n",
    "        \"Returns a generator of word-count pairs sorted by count.\"\n",
    "        return (wc for wc in sorted(self.counts.items(), key = lambda wc: wc[1], reverse=descending))\n",
    "\n",
    "def unzip(f):\n",
    "    if f.endswith(\".gz\"):\n",
    "        return gzip.open(f)\n",
    "    else:\n",
    "        return open(f, 'r')\n",
    "\n",
    "# Exercise 3: Remove stop words. Edit this set to taste!\n",
    "stop_words1 = {\n",
    "    'that', 'the', 'this', 'an',\n",
    "    'and', 'or', 'but', 'of'\n",
    "}\n",
    "## All the single digits and ASCII letters:\n",
    "l=[str(i) for i in range(10)]\n",
    "l.extend([chr(i) for i in range(ord('a'), ord('z')+1)])\n",
    "\n",
    "stop_words = stop_words1.union(set(l))\n",
    "\n",
    "def is_stop_word(word):\n",
    "    \"\"\"\n",
    "    Treat all single-character words, blanks, and integers as stop words.\n",
    "    (Try adding floating point numbers.)\n",
    "    Otherwise, check for membership in a set of words.\n",
    "    We use a set because it provides O(1) lookup!\n",
    "    \"\"\"\n",
    "    w = word.strip()\n",
    "    if len(w) <= 1 or w.isdigit():\n",
    "        return True\n",
    "    return w in stop_words\n",
    "\n",
    "def count_words(file_globs, top_n = 100, batch_window = 1024):\n",
    "    # The working directory of this application may be _different_\n",
    "    # than the Ray cluster's working directory. (In a real cluster,\n",
    "    # the files available will be different, too, but we'll ignore\n",
    "    # the problem here.) So, we need to pass absolute paths or our\n",
    "    # ray.util.iter.from_items won't find the files!\n",
    "    globs = [g for f in file_globs for g in glob.glob(f)]\n",
    "    file_list = list(map(lambda f: os.path.abspath(f), globs))\n",
    "\n",
    "    print(f'Processing {len(file_list)} files: {file_list}')\n",
    "    # Exercise 1: use combine instead of for_each(...).flatten(...).\n",
    "    # We replace two occurrences:\n",
    "    word_count = (\n",
    "        ray.util.iter.from_items(file_list, num_shards=4)\n",
    "           .combine(lambda f: unzip(f).readlines())\n",
    "           # Exercise 2: convert to lower case!\n",
    "           .combine(lambda line: re.split('\\W+', line.lower())) # split into words.\n",
    "           # Exercise 3: remove stop words.\n",
    "           .filter(lambda word: not is_stop_word(word))\n",
    "           .for_each(lambda word: (word, 1))\n",
    "           .batch(batch_window)\n",
    "    )\n",
    "    # Combine the dictionaries of counts across shards with a sliding window\n",
    "    # of \"batch_window\" lines.\n",
    "    wordCount = WordCount()\n",
    "    for shard_counts in word_count.gather_async():\n",
    "        for word, count in shard_counts:\n",
    "            wordCount(word, count)\n",
    "    sorted_list_iterator = wordCount.sort_counts()\n",
    "    return [sorted_list_iterator.__next__() for i in range(top_n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time word_counts = count_words(['../*.ipynb'], top_n=100)  # The notebooks are now in the parent directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Ray Crash Course - Ray Parallel Iterators - Exercise 4\n",
    "\n",
    "Now let's run `count_words` on the `README.md` for the tutorial repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time word_counts_readme = count_words(['../../README.md'], top_n=100)  # The README is two directories up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now which words are most prominent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\". Terminate all the processes started in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
